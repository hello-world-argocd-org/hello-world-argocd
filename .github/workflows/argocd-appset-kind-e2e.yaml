name: Argo CD ApplicationSet E2E (Kind)

on:
  push:
    branches: [ main ]
  pull_request:
  workflow_dispatch:

jobs:
  e2e:
    runs-on: ubuntu-latest
    env:
      APPSET_DIR: applicationsets
      ARGO_NS: argocd
      ARGO_HELM_CHART_VERSION: "5.51.6"
      WAIT_ROLLOUT: "240s"
      WAIT_APPS: "90"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install tools (Helm, yq, jq)
        run: |
          curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          sudo curl -fsSL -o /usr/local/bin/yq https://github.com/mikefarah/yq/releases/download/v4.44.3/yq_linux_amd64
          sudo chmod +x /usr/local/bin/yq
          sudo apt-get update -y && sudo apt-get install -y jq

      - name: Create Kind cluster
        uses: helm/kind-action@v1.8.0

      - name: Install Argo CD (ApplicationSet enabled)
        run: |
          set -euo pipefail
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo update
          kubectl create namespace "$ARGO_NS" --dry-run=client -o yaml | kubectl apply -f -
          helm upgrade --install argocd argo/argo-cd \
            --namespace "$ARGO_NS" \
            --version "$ARGO_HELM_CHART_VERSION" \
            --set applicationSet.enabled=true \
            --wait
          kubectl -n "$ARGO_NS" rollout status deploy/argocd-repo-server --timeout="$WAIT_ROLLOUT" || true

      - name: Show what we are going to apply
        run: |
          for f in $APPSET_DIR/*.yaml; do
            echo "---- $f ----"
            yq e '.kind + " " + .metadata.name' "$f" || true
          done

      - name: Apply ApplicationSet manifests
        run: |
          kubectl -n "$ARGO_NS" apply -f "$APPSET_DIR/"

      - name: Sanity — repo-server can reach GitHub and list refs
        run: |
          set -euo pipefail
          POD=$(kubectl -n "$ARGO_NS" get pod -l app.kubernetes.io/name=argocd-repo-server -o jsonpath='{.items[0].metadata.name}')
          CONTAINER=$(kubectl -n "$ARGO_NS" get pod "$POD" -o jsonpath='{.spec.containers[0].name}')
          kubectl -n "$ARGO_NS" exec "$POD" -c "$CONTAINER" -- sh -c 'getent hosts github.com || nslookup github.com || true'
          kubectl -n "$ARGO_NS" exec "$POD" -c "$CONTAINER" -- git ls-remote https://github.com/hello-world-argocd-org/hello-world-delivery.git | head -n1

      - name: Wait for Applications to be generated
        run: |
          set -euo pipefail
          end=$((SECONDS + WAIT_APPS))
          ok=0
          while [ $SECONDS -lt $end ]; do
            count=$(kubectl -n "$ARGO_NS" get applications.argoproj.io -o json 2>/dev/null | jq '.items | length')
            if [ "${count:-0}" -ge 1 ]; then
              echo "Found $count Application(s)."
              ok=1; break
            fi
            kubectl -n "$ARGO_NS" logs deploy/argocd-repo-server --tail=10 2>/dev/null || true
            sleep 5
          done
          kubectl -n "$ARGO_NS" get applications.argoproj.io || true
          if [ "$ok" -ne 1 ]; then
            echo "❌ No Applications were generated in ${WAIT_APPS}s."; exit 1
          fi

      - name: Verify application endpoints (dev/stage/prod)
        shell: bash
        run: |
          set -euo pipefail

            verify_ns () {
              local ns="$1"
              local expected="$2"
  
              echo "=== Checking namespace: $ns ==="
  
              # Find a ClusterIP Service that selects pods (has a selector) and has at least one port
              read -r SVC PORT <<EOF
              $(kubectl -n "$ns" get svc -o json | jq -r '
              .items[]
              | select(.spec.type=="ClusterIP")
              | select(.spec.selector!=null and (.spec.selector|length)>0)
              | select(.spec.ports!=null and (.spec.ports|length)>0)
              | "\(.metadata.name) \(.spec.ports[0].port)"
              ' | head -n1)
              EOF
        
                    if [[ -z "${SVC:-}" || -z "${PORT:-}" ]]; then
                      echo "::error:: No suitable Service found in namespace '$ns'." >&2
                      kubectl -n "$ns" get svc -o wide || true
                      exit 1
                    fi
          
              echo "Using Service:
              $SVC port: $PORT"
              
              # Wait for endpoints to be ready (up to 90s)
              end=$((SECONDS + 90))
              ready=0
              while [ $SECONDS -lt $end ]; do
                addrs=$(kubectl -n "$ns" get endpoints "$SVC" -o json 2>/dev/null | jq -r '[.subsets[]?.addresses[]?] | length' || echo 0)
                if [ "${addrs:-0}" -ge 1 ]; then ready=1; break; fi
                sleep 3
              done
              if [ "$ready" -ne 1 ]; then
                echo "::error:: Endpoints for $SVC are not ready in $ns."
                kubectl -n "$ns" get endpoints "$SVC" -o yaml || true
                exit 1
              fi
              
              # Port-forward and probe with retries
              kubectl -n "$ns" port-forward "svc/$SVC" 18080:"$PORT" >/tmp/pf-"$ns".log 2>&1 &
              pf_pid=$!
              
              # Give port-forward a moment, then try up to 20 times
              sleep 2
              ok=0
              for i in {1..20}; do
                if actual=$(curl -fsS http://127.0.0.1:18080/ 2>/dev/null); then
                  echo "Got: $actual"
                  if [[ "$actual" == "$expected" ]]; then
                    ok=1
                  fi
                break
              fi
              
              sleep 1
              done
              
              kill "$pf_pid" 2>/dev/null || true
              wait "$pf_pid" 2>/dev/null || true
              
              if [ "$ok" -ne 1 ]; then
              echo "::error:: Unexpected response in $ns"
              echo "Expected: $expected"
              echo "Actual: ${actual:-<no response>}"
              echo "Port-forward logs:"
              sed -n '1,120p' "/tmp/pf-$ns.log" || true
              exit 1
              fi
              
              echo "✓ $ns OK"
          }

          verify_ns dev   "Shared value: [5] env value: [DEV]"
          verify_ns stage "Shared value: [5] env value: [STAGE]"
          verify_ns prod  "Shared value: [5] env value: [PROD]"
